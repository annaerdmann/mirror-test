{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning notebook\n",
    "\n",
    "\n",
    "  Here we show how you can take our granite-geospatial-uki model which was pre-trained on satellite imagery over the UK and Ireland and fine-tune it for a specific task in a different region. As the model was pre-trained using data from UK and Ireland, performance may vary over different regions. However, as we will see in this example, the model can be successfully fine tuned with local data to perform well in a different location. The addition of Sentinel-1 synthetic aperture radar (SAR) data, which can see through clouds, is particularly beneficial for applications such as flood detection, when events often coincide with the presence of clouds.\n",
    "\n",
    "Here we will demonstrate fine tuning on a dataset consisting of flood events in the UK and Ireland, as used in the granite-geospatial-uki-flood-detection model, with the addition of events from Spain. We then run inference for the recent devastating flood event in Valencia on 31 October 2024.\n",
    "\n",
    "Labelled flood events in the form of flood extents have been retrieved from the [Copernicus Emergency Management service (CEMS)](https://emergency.copernicus.eu/) Portal and provided as a fine tuning dataset. To fine tune for a different region, you can follow the instructions below to find and prepare additional local training data.\n",
    "\n",
    "##### Technical requirements\n",
    "\n",
    "It's best to run this notebook on a machine with one or more GPUs. If this is not possible, you can reduce the amount of training data to shorten the training time, at a cost of reduced performance. You can also try reducing the batch size. \n",
    "\n",
    "##### Preparation of fine-tuning data\n",
    "- find a flood event of your choice on CEMS and download the zip files containing the relevant data. The flood outlines are available in vector format, so convert these to raster. \n",
    "- note the imagery dates from the CEMS website for the event. [Query Sentinel-1 and Sentinel-2 imagery](https://sentinelhub-py.readthedocs.io/en/latest/) from sentinel hub over the flood event region, within a 2 to 3 days window from the imagery date listed on the CEMS website. \n",
    "- process the data as described in the [model card](../granite-geospatial-flood-detection-uki-model-card.md).\n",
    "\n",
    "For this particular example, we curated flood imagery from the [south east of Spain](https://emergency.copernicus.eu/mapping/list-of-components/EMSR388), where flooding was seen across a broad area in late September 2019. We fine-tune on a dataset that combines this Spanish flood with floods from the UK and Ireland.\n",
    "\n",
    "  ## 0. Setting up your environment\n",
    "  The granite-geospatial-uki-flood-detection model is trained using the [Terratorch](https://github.com/IBM/terratorch/) framework. Terratorch simplifies the integration of foundation model backbones into the TorchGeo framework and provides generic LightningDataModules that can be built at runtime. For more information on Terratorch, please refer to the instructions in the [Terratorch Quick Start Guide](https://github.com/IBM/terratorch/blob/main/docs/quick_start.md). To set up your environments for running this notebook on Google Colab, please see section 0.1. If you plan to run this notebook on a local machine please see section 0.2 instead for set-up instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 set-up for running on Google Colab\n",
    "\n",
    "You may want to take this opportunity to double check you're using GPUs on Google Colab before proceeding any further. We have tested this notebook using T4 GPU on the free colab account. \n",
    "\n",
    "#### 0.1.1 check python version\n",
    "It's recommended that you run this notebook using python 3.10. Let's check the python version by executing the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1.2 setting up your environment\n",
    "To install the necessary packages, execute the cell below. This will take a few minutes. Once the installation process is done, a window will pop up to ask you to restart the session. This is normal and you should proceed to restart using the interface in the pop up window. Once the session has restarted, it's important that you ignore the cell below, and go straight to section 0.0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# if running on colab\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Clone the ibm-granite/geospatial/uki-flooddetection GitHub #TODO - confirm name when ready\n",
    "    !git clone ...   #TODO - change to correct location when ready\n",
    "    # Install the package\n",
    "    %pip install -e ./granite-geospatial-flooddetection-uki[colab]   #TODO - change to correct location when ready\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.0.3 Set up working directory\n",
    "This is the first thing you should run after restarting your Colab session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Only run this cell if running on Colab.\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Change to the notebooks directory\n",
    "    %cd granite-geospatial-flooddetection-uki/notebooks\n",
    "    # TODO - change to correct location when ready\n",
    "    %pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your environment is set up for Google Colab. Please proceed to section 0.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 set-up for running on your local machine\n",
    "\n",
    "Before running through this notebook it's best to create a virtual environment and install the necessary packages there before running this notebook. The instructions can be found in [README.md](../README.md).\n",
    "\n",
    "  Once that's done, come back to this notebook and make sure it's using the newly made virtual environment.\n",
    "\n",
    "  Please proceed to section 0.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 importing packages and setting up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "\n",
    "import rioxarray\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from granite_geo_flood.utils.helper import (\n",
    "    download_data,\n",
    "    prep_valencia_images,\n",
    "    plot_images_pred_valencia,\n",
    "    clip_image,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic set-up\n",
    "%matplotlib inline\n",
    "\n",
    "project_root = Path(\"../\")\n",
    "hf_repo_name = ...  # TODO - to fill in when ready\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login  # TODO - delete when ready\n",
    "\n",
    "login()  # TODO - delete when ready\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 1. Fine-tuning \n",
    "\n",
    "  We'll go through the fine-tunining process now. If you encounter any problems, you can skip this section and download the fine-tuned weights to carry out inference in section 2.2.\n",
    "\n",
    "  ### 1.1 Data prep\n",
    "  Let's place this in `../data/regions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify where the training data are stored\n",
    "data_path = project_root / \"data\" / \"regions\"\n",
    "os.makedirs(data_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and uncompress full-tile dataset\n",
    "save_file = project_root / \"data\" / \"granite-geospatial-uki-flooddetection-dataset-combined-uki-spain.tar.gz\"\n",
    "download_data(\"uki_and_spain\", save_file)\n",
    "command = f'tar -xf \"{save_file}\" --directory \"{data_path}\"'\n",
    "os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 1.2 Model prep - checkpoints\n",
    "\n",
    "\n",
    "\n",
    "  We can get the pre-trained weights from HuggingFace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint-specific\n",
    "checkpoint_name = \"granite_geospatial_uki.pt\"\n",
    "checkpoint_folder = project_root / \"data\" / \"checkpoints\"\n",
    "\n",
    "inference_checkpoint = Path(\n",
    "    hf_hub_download(\n",
    "        repo_id=hf_repo_name,\n",
    "        filename=checkpoint_name,\n",
    "        local_dir=checkpoint_folder,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 1.3 Model prep - configs\n",
    "\n",
    "\n",
    "\n",
    "  As this model uses additional bands to the prithvi model included in terratorch, we allow for the weights for these additional bands to be read in by terratorch by defining a custom module in `./custom_modules/granite_geospatial_uki.py`.\n",
    "\n",
    "\n",
    "\n",
    "  We make sure to point to this as a backbone in our config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download model config\n",
    "config_name = \"config_granite_geospatial_uki_flood_detection_v2.yaml\"\n",
    "config_folder = project_root / \"configs\"\n",
    "\n",
    "config_file = Path(\n",
    "    hf_hub_download(\n",
    "        repo_id=hf_repo_name,\n",
    "        filename=config_name,\n",
    "        local_dir=config_folder,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note: the `batch_size` parameter is set to 4 and `max_epochs` has been set to 5 to avoid running out of memory or runtime for users of the free tier colab compute resources. This is enough to demonstrate the entire workflow to the user, but may not result in the best performance. It'll be best to find additional compute resources and increase `batch_size` and `max_epochs` in the downloaded config file for improved performance. For reference, the `granite_geospatial_uki_flood_detection_v2.ckpt` checkpoint in section 2.2 was obtained by fine-tuning on a `batch_size` of 16 and using early stopping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 1.4 Carry out fine-tuning\n",
    "\n",
    "  Execute the below cell to print out a command. Check the command and the config location to make sure that the config file exists in the expected folder. \n",
    "\n",
    "As touched upon before, **granite-geospatial-uki** is not a standard model in Terratorch. To load the model (or any custom backbone) we can put the model file in a folder called `custom_modules`. In this example the file is called `granite_geospatial_uki.py`. \n",
    "\n",
    " If you're planning to run this command outside of this notebook when applying it to your own project, it's important to **make sure you're running terratorch from the directory where the `custom_modules` folder is located**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_command = f\"terratorch fit --config ./{config_file}\"\n",
    "print(fine_tuning_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything looks ok, we'll execute the below cell to fine-tune the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(fine_tuning_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 2. Checking the results - inference prep\n",
    "\n",
    "\n",
    "\n",
    "  Let's gather and specify the relevant files for carrying out inference in a new folder. \n",
    "  Look for your .ckpt file produced during the fine-tuning process and list it in the cell in section 2.1. \n",
    "  If you didn't fine-tune the model or you only fine-tuned ir for a small number of epochs, you can still carry out inference by downloading the fine-tuned weights from Hugging Face. Skip section 2.1 and execute section 2.2 if you'd like to do that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 inference checkpoint specification if you fine-tuned yourself\n",
    "\n",
    "Only execute the cell below if you're happy that you've ran the fine-tuning for long enough that you've got a checkpoint from the \"best\" epoch. \n",
    "\n",
    "If you've used the default epoch number listed in the Hugging Face config (e.g. when running fine-tuning using the free tier colab compute resources) it's best to skip the cell below. If you head to section 2.2 instead, you can download the fully trained weights and carry out inference on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the checkpoint produced from the fine-tuning process, and overwrite below\n",
    "inference_checkpoint = (\n",
    "    project_root\n",
    "    / \"data\"\n",
    "    / \"fine_tuning\"\n",
    "    / \"granite_geospatial_uki_flood_detection_v2\"\n",
    "    / \"lightning_logs\"\n",
    "    / \"version_0\"\n",
    "    / \"checkpoints\"\n",
    "    / \"epoch=4.ckpt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 inference checkpoint download if you didn't fine-tune yourself or you fine-tuned only for a few epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = \"granite_geospatial_uki_flood_detection_v2.ckpt\"\n",
    "checkpoint_folder = project_root / \"data\" / \"checkpoints\"\n",
    "\n",
    "inference_checkpoint = Path(\n",
    "    hf_hub_download(\n",
    "        repo_id=hf_repo_name,\n",
    "        filename=checkpoint_name,\n",
    "        local_dir=checkpoint_folder,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_checkpoint_name = \"granite_geospatial_uki.pt\"\n",
    "\n",
    "pretrain_checkpoint = Path(\n",
    "    hf_hub_download(\n",
    "        repo_id=hf_repo_name,\n",
    "        filename=pretrain_checkpoint_name,\n",
    "        local_dir=checkpoint_folder,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 preparing paths and inference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify where you want to save your inference results\n",
    "inference_dir = project_root / \"data\" / \"inference\" / \"valencia\"\n",
    "\n",
    "# specify where the test images are stored\n",
    "data_dir = project_root / \"data\" / \"regions\" / \"valencia\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download the inferece data \n",
    "save_file = project_root / \"data\" / \"granite-geospatial-uki-flooddetection-dataset-valencia.tar.gz\"\n",
    "download_data(\"valencia\", save_file)\n",
    "command = f'tar -xf \"{str(save_file)}\" --directory \"{str(data_path)}\"'\n",
    "os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running this notebook using the free tier colab compute resources, please execute the cell below. This cell will clip the images on which we plan to run inference to a smaller size, to avoid running out of memory. Please note, this will overwrite the downloaded images. If you're running this notebook on other compute you can likely skip the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_before_flood = \"../data/regions/valencia/valencia_2024-09-30.tif\"\n",
    "image_before_flood = rioxarray.open_rasterio(file_before_flood)\n",
    "image_before_flood = clip_image(image_before_flood)\n",
    "\n",
    "file_after_flood = \"../data/regions/valencia/valencia_2024-10-31.tif\"\n",
    "image_after_flood = rioxarray.open_rasterio(file_after_flood)\n",
    "image_after_flood = clip_image(image_after_flood)\n",
    "\n",
    "# write out clipped data\n",
    "image_before_flood.rio.write_crs(\"epsg:4326\", inplace=True)\n",
    "image_before_flood = image_before_flood.squeeze()\n",
    "image_before_flood.rio.to_raster(file_before_flood)\n",
    "image_after_flood.rio.write_crs(\"epsg:4326\", inplace=True)\n",
    "image_after_flood = image_after_flood.squeeze()\n",
    "image_after_flood.rio.to_raster(file_after_flood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's carry out inference on the test images. Execute the cell below to print out a command. Make sure the paths look correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference\n",
    "inference_command = f\"terratorch predict -c ./{config_file} --ckpt_path ./{inference_checkpoint} --predict_output_dir ./{inference_dir} --data.init_args.predict_data_root ./{data_dir}\"\n",
    "print(inference_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything looks good, execute the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(inference_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 3. Checking the results - visualisation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices of RGB bands, in order, in the input images of size (h x w x bands)\n",
    "rgb_bands = [\n",
    "    4,\n",
    "    3,\n",
    "    2,\n",
    "]\n",
    "\n",
    "# index of VV band, in the input images of size (h x w x bands)\n",
    "vv_band = 0\n",
    "\n",
    "save_dir = project_root / \"plots\" / \"valencia\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# defining where to save the image\n",
    "save_file = save_dir / \"inference.png\"\n",
    "\n",
    "# find the relevant data\n",
    "image_file_before_flood = data_dir / \"valencia_2024-09-30.tif\"\n",
    "image_before_flood = rioxarray.open_rasterio(image_file_before_flood)\n",
    "\n",
    "image_file_after_flood = data_dir / \"valencia_2024-10-31.tif\"\n",
    "image_after_flood = rioxarray.open_rasterio(image_file_after_flood)\n",
    "\n",
    "pred_file_before_flood = inference_dir / \"valencia_2024-09-30_pred.tif\"\n",
    "pred_before_flood = rioxarray.open_rasterio(pred_file_before_flood)\n",
    "\n",
    "pred_file_after_flood = inference_dir / \"valencia_2024-10-31_pred.tif\"\n",
    "pred_after_flood = rioxarray.open_rasterio(pred_file_after_flood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep valencia data for plotting\n",
    "s1_before_flood, s2_before_flood, pred_before_flood = prep_valencia_images(\n",
    "    image_before_flood, pred_before_flood, rgb_bands, vv_band\n",
    ")\n",
    "s1_after_flood, s2_after_flood, pred_after_flood = prep_valencia_images(\n",
    "    image_after_flood, pred_after_flood, rgb_bands, vv_band\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find dates of floods \n",
    "pattern = \".*_(\\\\d{4}-\\\\d{2}-\\\\d{2}).*\"\n",
    "flood_date_before = re.findall(pattern, str(image_file_before_flood))[0]\n",
    "flood_date_after = re.findall(pattern, str(image_file_after_flood))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_pred_valencia(\n",
    "    s1_before_flood,\n",
    "    s2_before_flood,\n",
    "    pred_before_flood,\n",
    "    s1_after_flood,\n",
    "    s2_after_flood,\n",
    "    flood_date_before,\n",
    "    flood_date_after,\n",
    "    pred_after_flood,\n",
    "    save_file,\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the inputs and output of the flood detection model fine tuned for Spain and used to delineate flooded areas in Valencia using data from both Sentinel 1 and Sentinel 2 on 31 October 2024. The Sentinel-2 image is partly obscured by clouds, especially to the south. Here, the model relies on Sentinel-1 to delineate the flooded area.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "Check out the other granite-geospatial models for [Above Ground Biomass](https://huggingface.co/ibm-granite/granite-geospatial-biomass), [Canopy Height](https://huggingface.co/ibm-granite/granite-geospatial-canopyheight), [Land Surface Temperature and Weather and Climate Downscaling](https://huggingface.co/ibm-granite/granite-geospatial-wxc-downscaling). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "granite-geospatial-flooddetection-uki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
